[strings]
# Mode : train, test, serve
# train_data_path = ./data/ccks/train_ch_demo.tsv
# test_data_path = ./data/ccks/test_ch_demo.tsv
# dev_data_path = ./data/ccks/test_ch_demo.tsv
# rel_file = ./data/ccks/relation_label.txt
train_data_path = ./data/relation_data/train_data.txt
test_data_path = ./data/relation_data/test_data.txt
dev_data_path = ./data/relation_data/test_data.txt
rel_file = ./data/relation_data/relation.txt

# model evaluate result
output_evaluate_path = ./result/
output_evaluate_file = model_evaluate_result.json

# Pretrain model
# pretrain_model_path = D:/pretrain_model/torch/bert-base-chinese/
# pretrain_model_type = bert-base
# pretrain_model_path = D:/Spyder/pretrain_model/transformers_torch_tf/bert-base-chinese/
pretrain_model_path = D:/Spyder/pretrain_model/transformers_torch_tf/chinese-roberta-wwm-ext/
pretrain_model_type = bert-base
# pretrain_model_path = /home/pretrain_model/chinese-roberta-wwm-ext/
# pretrain_model_type = bert-base

# gpu ids
visible_gpus = 4
# save para
output_path = ./output/
model_name = model.pt
replace_char = â™ 
hidden_type = last_hidden_state
pooler_type = pooler
[ints]
# model para
max_seq_length = 128
epochs = 50
batch_size = 16
dev_batch_size = 8
seed = 1234
local_rank = 0
eval_interval = 2
require_improvement = 500
pre_epoch_step_print = 1000
gradient_accumulation_steps = 1
warmup_steps = 0
max_steps = -1

[floats]
learning_rate = 1e-5
weight_decay = 0.0
dropout_rate = 0.1
adam_epsilon = 1e-8
max_grad_norm = 1.0

[bools]
is_test = False
